#!/usr/bin/env python3
"""
å®Œæ•´æ”¹é€²ç‰ˆæœ¬ï¼š
1. æ“´å±•æ‰€æœ‰ç›®æ¨™è®Šæ•¸ (Tg, FFV, Tc, Density, Rg)
2. æ·»åŠ å®Œæ•´åˆ†å­æè¿°ç¬¦
3. çµ„åˆæœ€ä½³æŒ‡ç´‹ (Morgan + MACCS)
4. äº¤å‰é©—è­‰è©•ä¼°
5. å®Œæ•´è©•ä¼°æŒ‡æ¨™
"""

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem, MACCSkeys, Descriptors, rdMolDescriptors
from rdkit.Chem.MACCSkeys import GenMACCSKeys
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ å®Œæ•´æ”¹é€²ç‰ˆèšåˆç‰©æ€§è³ªé æ¸¬æ¨¡å‹")
print("=" * 60)
print("âœ… æ“´å±•æ‰€æœ‰ç›®æ¨™è®Šæ•¸")
print("âœ… çµ„åˆæŒ‡ç´‹ (Morgan + MACCS)")
print("âœ… å®Œæ•´åˆ†å­æè¿°ç¬¦")
print("âœ… äº¤å‰é©—è­‰è©•ä¼°")
print("âœ… å®Œæ•´è©•ä¼°æŒ‡æ¨™")
print("=" * 60)

# ===============================
# 1. è³‡æ–™è¼‰å…¥èˆ‡åŸºæœ¬è™•ç†
# ===============================
print("\nğŸ“ è¼‰å…¥è³‡æ–™...")
train = pd.read_csv("./data/train.csv")
test = pd.read_csv("./data/test.csv")

print(f"è¨“ç·´é›†å¤§å°: {train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {test.shape}")

# SMILES è½‰åˆ†å­
train["mol"] = train["SMILES"].apply(Chem.MolFromSmiles)
test["mol"] = test["SMILES"].apply(Chem.MolFromSmiles)

# ===============================
# 2. ç‰¹å¾µæå–å‡½æ•¸å®šç¾©
# ===============================
print("\nğŸ§¬ å®šç¾©ç‰¹å¾µæå–å‡½æ•¸...")

def compute_morgan_fp(mol, radius=2, nBits=2048):
    """è¨ˆç®—MorganæŒ‡ç´‹"""
    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits))

def compute_maccs_fp(mol):
    """è¨ˆç®—MACCSæŒ‡ç´‹"""
    return np.array(GenMACCSKeys(mol))

def compute_comprehensive_descriptors(mol):
    """è¨ˆç®—å®Œæ•´çš„åˆ†å­æè¿°ç¬¦é›†åˆ"""
    desc = []
    
    # ===================
    # åŸºæœ¬ç‰©ç†åŒ–å­¸æ€§è³ª
    # ===================
    desc.append(Descriptors.MolWt(mol))                    # åˆ†å­é‡
    desc.append(Descriptors.ExactMolWt(mol))               # ç²¾ç¢ºåˆ†å­é‡
    desc.append(mol.GetNumHeavyAtoms())                    # é‡åŸå­æ•¸
    desc.append(mol.GetNumAtoms())                         # ç¸½åŸå­æ•¸
    desc.append(Descriptors.NumValenceElectrons(mol))      # åƒ¹é›»å­æ•¸
    
    # ===================
    # ç–æ°´æ€§èˆ‡æ¥µæ€§
    # ===================
    desc.append(Descriptors.MolLogP(mol))                  # ç–æ°´æ€§ä¿‚æ•¸
    desc.append(Descriptors.MolMR(mol))                    # æ‘©çˆ¾æŠ˜å°„ç‡
    desc.append(rdMolDescriptors.CalcTPSA(mol))            # æ‹“æ’²æ¥µæ€§è¡¨é¢ç©
    
    # ===================
    # æ°«éµç‰¹æ€§
    # ===================
    desc.append(Descriptors.NumHDonors(mol))               # æ°«éµä¾›é«”æ•¸
    desc.append(Descriptors.NumHAcceptors(mol))            # æ°«éµå—é«”æ•¸
    
    # ===================
    # çµæ§‹æŸ”æ€§
    # ===================
    desc.append(Descriptors.NumRotatableBonds(mol))        # å¯æ—‹è½‰éµæ•¸
    desc.append(rdMolDescriptors.CalcFractionCSP3(mol))    # sp3ç¢³æ¯”ä¾‹
    
    # ===================
    # ç’°ç³»çµ±ç‰¹å¾µ
    # ===================
    desc.append(Descriptors.NumRings(mol))                 # ç’°ç¸½æ•¸
    desc.append(Descriptors.NumAromaticRings(mol))         # èŠ³é¦™ç’°æ•¸
    desc.append(Descriptors.NumSaturatedRings(mol))        # é£½å’Œç’°æ•¸
    desc.append(Descriptors.NumAliphaticRings(mol))        # è„‚è‚ªç’°æ•¸
    
    # ===================
    # åˆ†å­è¤‡é›œåº¦
    # ===================
    desc.append(Descriptors.BertzCT(mol))                  # Bertzè¤‡é›œåº¦æŒ‡æ•¸
    desc.append(Descriptors.HallKierAlpha(mol))            # Hall-Kier AlphaæŒ‡æ•¸
    desc.append(Descriptors.BalabanJ(mol))                 # Balaban JæŒ‡æ•¸
    
    # ===================
    # é€£é€šæ€§æŒ‡æ•¸ (Chi indices)
    # ===================
    desc.append(Descriptors.Chi0(mol))                     # Chi0 é€£é€šæ€§æŒ‡æ•¸
    desc.append(Descriptors.Chi0n(mol))                    # Chi0n
    desc.append(Descriptors.Chi0v(mol))                    # Chi0v
    desc.append(Descriptors.Chi1(mol))                     # Chi1
    desc.append(Descriptors.Chi1n(mol))                    # Chi1n
    desc.append(Descriptors.Chi1v(mol))                    # Chi1v
    
    # ===================
    # Kappa å½¢ç‹€æŒ‡æ•¸
    # ===================
    desc.append(Descriptors.Kappa1(mol))                   # Kappa1
    desc.append(Descriptors.Kappa2(mol))                   # Kappa2
    desc.append(Descriptors.Kappa3(mol))                   # Kappa3
    
    # ===================
    # èšåˆç‰©ç‰¹æœ‰æè¿°ç¬¦
    # ===================
    desc.append(Descriptors.FractionCarbons(mol))          # ç¢³åŸå­æ¯”ä¾‹
    desc.append(Descriptors.NumHeteroatoms(mol))           # é›œåŸå­æ•¸
    desc.append(mol.GetNumBonds())                         # éµæ•¸
    
    # è™•ç†å¯èƒ½çš„NaNå€¼
    desc = [0.0 if (pd.isna(x) or np.isinf(x)) else float(x) for x in desc]
    
    return np.array(desc)

# ===============================
# 3. è¨ˆç®—æ‰€æœ‰ç‰¹å¾µ
# ===============================
print("\nâš™ï¸  è¨ˆç®—ç‰¹å¾µçŸ©é™£...")

# æŒ‡ç´‹ç‰¹å¾µ
print("  è¨ˆç®—MorganæŒ‡ç´‹...")
morgan_fps = np.array([compute_morgan_fp(mol) for mol in train["mol"]])

print("  è¨ˆç®—MACCSæŒ‡ç´‹...")
maccs_fps = np.array([compute_maccs_fp(mol) for mol in train["mol"]])

print("  è¨ˆç®—åˆ†å­æè¿°ç¬¦...")
descriptors = np.array([compute_comprehensive_descriptors(mol) for mol in train["mol"]])

# çµ„åˆæ‰€æœ‰ç‰¹å¾µ
print("  çµ„åˆç‰¹å¾µ...")
X_combined = np.hstack([
    morgan_fps,     # 2048ç¶­
    maccs_fps,      # 167ç¶­
    descriptors     # 28ç¶­
])

print(f"âœ… æœ€çµ‚ç‰¹å¾µçŸ©é™£ç¶­åº¦: {X_combined.shape}")
print(f"   - MorganæŒ‡ç´‹: {morgan_fps.shape[1]} ç¶­")
print(f"   - MACCSæŒ‡ç´‹: {maccs_fps.shape[1]} ç¶­") 
print(f"   - åˆ†å­æè¿°ç¬¦: {descriptors.shape[1]} ç¶­")
print(f"   - ç¸½è¨ˆ: {X_combined.shape[1]} ç¶­")

# ===============================
# 4. ç›®æ¨™è®Šæ•¸æº–å‚™
# ===============================
targets = ["Tg", "FFV", "Tc", "Density", "Rg"]
y_data = train[targets].copy()

print(f"\nğŸ“Š ç›®æ¨™è®Šæ•¸çµ±è¨ˆ:")
for target in targets:
    missing = y_data[target].isnull().sum()
    available = y_data[target].notnull().sum()
    percentage = (available / len(y_data)) * 100
    print(f"  {target:8s}: å¯ç”¨ {available:4d} ({percentage:5.1f}%), ç¼ºå¤± {missing:4d}")

# ===============================
# 5. æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°
# ===============================
print(f"\nğŸ¤– é–‹å§‹è¨“ç·´æ¨¡å‹...")
print("=" * 60)

# æ”¹é€²çš„RandomForeståƒæ•¸
improved_rf = RandomForestRegressor(
    n_estimators=300,        # å¢åŠ æ¨¹æ•¸é‡æå‡æ€§èƒ½
    max_depth=20,           # é™åˆ¶æ·±åº¦é˜²æ­¢éæ“¬åˆ
    min_samples_split=5,    # æœ€å°åˆ†å‰²æ¨£æœ¬æ•¸
    min_samples_leaf=2,     # è‘‰ç¯€é»æœ€å°æ¨£æœ¬æ•¸
    max_features='sqrt',    # éš¨æ©Ÿç‰¹å¾µé¸æ“‡
    bootstrap=True,         # è‡ªåŠ©æ¡æ¨“
    random_state=42,
    n_jobs=-1              # ä¸¦è¡Œè¨ˆç®—
)

# çµæœå„²å­˜
results = []
detailed_results = {}

# äº¤å‰é©—è­‰è¨­å®š
cv_folds = 5
cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

for target in targets:
    print(f"\nğŸ¯ è™•ç†ç›®æ¨™è®Šæ•¸: {target}")
    print("-" * 40)
    
    # å–å¾—è©²ç›®æ¨™çš„éç©ºæ¨£æœ¬
    mask = y_data[target].notnull()
    X_target = X_combined[mask]
    y_target = y_data[target][mask]
    
    n_samples = len(y_target)
    print(f"å¯ç”¨æ¨£æœ¬æ•¸: {n_samples}")
    
    if n_samples < 50:  # æ¨£æœ¬å¤ªå°‘è·³éäº¤å‰é©—è­‰
        print("âš ï¸  æ¨£æœ¬æ•¸å¤ªå°‘ï¼Œè·³éè©²ç›®æ¨™è®Šæ•¸")
        continue
    
    # ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_target_scaled = scaler.fit_transform(X_target)
    
    # =========================
    # Hold-outé©—è­‰
    # =========================
    X_train, X_test, y_train, y_test = train_test_split(
        X_target_scaled, y_target, test_size=0.2, random_state=42
    )
    
    # è¨“ç·´æ¨¡å‹
    model = RandomForestRegressor(**improved_rf.get_params())
    model.fit(X_train, y_train)
    
    # é æ¸¬
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # =========================
    # äº¤å‰é©—è­‰
    # =========================
    print("  åŸ·è¡Œäº¤å‰é©—è­‰...")
    cv_r2_scores = cross_val_score(model, X_target_scaled, y_target, 
                                  cv=cv, scoring='r2', n_jobs=-1)
    cv_neg_mse_scores = cross_val_score(model, X_target_scaled, y_target, 
                                       cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)
    cv_neg_mae_scores = cross_val_score(model, X_target_scaled, y_target, 
                                       cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)
    
    # =========================
    # è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™
    # =========================
    
    # Hold-outæŒ‡æ¨™
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    train_rmse = np.sqrt(train_mse)
    test_rmse = np.sqrt(test_mse)
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    # äº¤å‰é©—è­‰æŒ‡æ¨™
    cv_r2_mean = cv_r2_scores.mean()
    cv_r2_std = cv_r2_scores.std()
    cv_mse_mean = -cv_neg_mse_scores.mean()
    cv_mse_std = cv_neg_mse_scores.std()
    cv_rmse_mean = np.sqrt(cv_mse_mean)
    cv_mae_mean = -cv_neg_mae_scores.mean()
    cv_mae_std = cv_neg_mae_scores.std()
    
    # ç›¸å°æŒ‡æ¨™
    y_std = y_target.std()
    relative_rmse = test_rmse / y_std
    relative_mae = test_mae / y_std
    
    # éæ“¬åˆæª¢æŸ¥
    overfitting = train_r2 - test_r2
    
    # å„²å­˜çµæœ
    result = {
        'target': target,
        'n_samples': n_samples,
        
        # Hold-outé©—è­‰çµæœ
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_mse': train_mse,
        'test_mse': test_mse,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_mae': train_mae,
        'test_mae': test_mae,
        
        # äº¤å‰é©—è­‰çµæœ
        'cv_r2_mean': cv_r2_mean,
        'cv_r2_std': cv_r2_std,
        'cv_mse_mean': cv_mse_mean,
        'cv_mse_std': cv_mse_std,
        'cv_rmse_mean': cv_rmse_mean,
        'cv_mae_mean': cv_mae_mean,
        'cv_mae_std': cv_mae_std,
        
        # ç›¸å°æŒ‡æ¨™
        'target_std': y_std,
        'relative_rmse': relative_rmse,
        'relative_mae': relative_mae,
        
        # æ¨¡å‹è¨ºæ–·
        'overfitting': overfitting,
    }
    
    results.append(result)
    detailed_results[target] = result
    
    # é¡¯ç¤ºé—œéµæŒ‡æ¨™
    print(f"  Hold-out RÂ²: {test_r2:.4f}")
    print(f"  äº¤å‰é©—è­‰ RÂ²: {cv_r2_mean:.4f} Â± {cv_r2_std:.4f}")
    print(f"  ç›¸å°RMSE: {relative_rmse:.4f}")
    print(f"  éæ“¬åˆç¨‹åº¦: {overfitting:.4f}")

# ===============================
# 6. çµæœåˆ†æèˆ‡è©•ä¼°
# ===============================
print(f"\n" + "=" * 80)
print("ğŸ“Š å®Œæ•´æ¨¡å‹è©•ä¼°å ±å‘Š")
print("=" * 80)

# è½‰æ›ç‚ºDataFrameä¾¿æ–¼åˆ†æ
results_df = pd.DataFrame(results)

print(f"\nğŸ† æ¨¡å‹æ€§èƒ½ç¸½è¦½")
print("-" * 50)
print(f"{'ç›®æ¨™':8s} {'æ¨£æœ¬æ•¸':>6s} {'Hold-out RÂ²':>12s} {'CV RÂ²':>15s} {'ç›¸å°RMSE':>10s} {'éæ“¬åˆ':>8s}")
print("-" * 50)

for _, row in results_df.iterrows():
    print(f"{row['target']:8s} "
          f"{row['n_samples']:6.0f} "
          f"{row['test_r2']:12.4f} "
          f"{row['cv_r2_mean']:8.4f}Â±{row['cv_r2_std']:5.3f} "
          f"{row['relative_rmse']:10.4f} "
          f"{row['overfitting']:8.4f}")

# åŸºæº–çµæœæ¯”è¼ƒ (ä¾†è‡ªåŸå§‹æ¨¡å‹)
baseline_results = {
    'Tg': 0.4573, 'FFV': 0.6520, 'Tc': 0.7046, 
    'Density': 0.7401, 'Rg': 0.6705
}

print(f"\nğŸ“ˆ èˆ‡åŸºæº–æ¨¡å‹æ¯”è¼ƒ")
print("-" * 50)
print(f"{'ç›®æ¨™':8s} {'åŸºæº–RÂ²':>8s} {'æ”¹é€²RÂ²':>8s} {'æå‡':>8s} {'æå‡%':>8s}")
print("-" * 50)

total_improvement = 0
valid_comparisons = 0

for _, row in results_df.iterrows():
    target = row['target']
    if target in baseline_results:
        baseline = baseline_results[target]
        improved = row['test_r2']
        improvement = improved - baseline
        improvement_pct = (improvement / baseline) * 100
        
        total_improvement += improvement_pct
        valid_comparisons += 1
        
        print(f"{target:8s} "
              f"{baseline:8.4f} "
              f"{improved:8.4f} "
              f"{improvement:+8.4f} "
              f"{improvement_pct:+7.1f}%")

if valid_comparisons > 0:
    avg_improvement = total_improvement / valid_comparisons
    print(f"{'å¹³å‡':8s} {'':8s} {'':8s} {'':8s} {avg_improvement:+7.1f}%")

print(f"\nğŸ” æ¨¡å‹å“è³ªè©•ä¼°")
print("-" * 50)

# å“è³ªç­‰ç´šåˆ†é¡
def classify_performance(r2, relative_rmse, overfitting):
    """æ ¹æ“šå¤šå€‹æŒ‡æ¨™åˆ†é¡æ¨¡å‹å“è³ª"""
    if r2 >= 0.8 and relative_rmse <= 0.3 and abs(overfitting) <= 0.05:
        return "å„ªç§€"
    elif r2 >= 0.6 and relative_rmse <= 0.5 and abs(overfitting) <= 0.1:
        return "è‰¯å¥½"
    elif r2 >= 0.4 and relative_rmse <= 0.7 and abs(overfitting) <= 0.15:
        return "å¯æ¥å—"
    else:
        return "éœ€æ”¹é€²"

print(f"{'ç›®æ¨™':8s} {'RÂ²ç­‰ç´š':>8s} {'RMSEç­‰ç´š':>10s} {'éæ“¬åˆ':>8s} {'ç¸½è©•':>8s}")
print("-" * 50)

for _, row in results_df.iterrows():
    r2_level = "å„ªç§€" if row['test_r2'] >= 0.8 else "è‰¯å¥½" if row['test_r2'] >= 0.6 else "å¯æ¥å—" if row['test_r2'] >= 0.4 else "å·®"
    rmse_level = "å„ªç§€" if row['relative_rmse'] <= 0.3 else "è‰¯å¥½" if row['relative_rmse'] <= 0.5 else "å¯æ¥å—" if row['relative_rmse'] <= 0.7 else "å·®"
    overfitting_status = "æ­£å¸¸" if abs(row['overfitting']) <= 0.1 else "éæ“¬åˆ" if row['overfitting'] > 0.1 else "æ¬ æ“¬åˆ"
    overall = classify_performance(row['test_r2'], row['relative_rmse'], row['overfitting'])
    
    print(f"{row['target']:8s} "
          f"{r2_level:>8s} "
          f"{rmse_level:>10s} "
          f"{overfitting_status:>8s} "
          f"{overall:>8s}")

print(f"\nğŸ’¡ å»ºè­°èˆ‡ä¸‹ä¸€æ­¥")
print("-" * 50)

# è‡ªå‹•ç”Ÿæˆå»ºè­°
best_target = results_df.loc[results_df['test_r2'].idxmax(), 'target']
worst_target = results_df.loc[results_df['test_r2'].idxmin(), 'target']

print(f"âœ… è¡¨ç¾æœ€ä½³: {best_target} (RÂ² = {results_df['test_r2'].max():.4f})")
print(f"âš ï¸  éœ€è¦æ”¹é€²: {worst_target} (RÂ² = {results_df['test_r2'].min():.4f})")

# æ ¹æ“šçµæœçµ¦å‡ºå…·é«”å»ºè­°
avg_r2 = results_df['test_r2'].mean()
if avg_r2 >= 0.7:
    print("ğŸ‰ æ¨¡å‹æ•´é«”è¡¨ç¾å„ªç§€ï¼å»ºè­°ï¼š")
    print("   - å¯ä»¥å˜—è©¦é›†æˆå­¸ç¿’é€²ä¸€æ­¥æå‡")
    print("   - è€ƒæ…®è¶…åƒæ•¸ç²¾èª¿")
elif avg_r2 >= 0.5:
    print("ğŸ‘ æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ï¼š")
    print("   - å˜—è©¦XGBoost/LightGBMç®—æ³•")
    print("   - æ·»åŠ æ›´å¤šé ˜åŸŸç‰¹å¾µ")
    print("   - è€ƒæ…®ç‰¹å¾µé¸æ“‡")
else:
    print("ğŸ”§ æ¨¡å‹éœ€è¦æ”¹é€²ï¼Œå»ºè­°ï¼š")
    print("   - æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹")
    print("   - å˜—è©¦ä¸åŒç®—æ³•")
    print("   - å¢åŠ æ•¸æ“šé‡")

# éæ“¬åˆè¨ºæ–·
overfitting_targets = results_df[results_df['overfitting'] > 0.1]['target'].tolist()
if overfitting_targets:
    print(f"âš ï¸  ç™¼ç¾éæ“¬åˆ: {', '.join(overfitting_targets)}")
    print("   - å»ºè­°æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦æˆ–å¢åŠ æ­£å‰‡åŒ–")

# å„²å­˜çµæœ
results_df.to_csv("comprehensive_model_results.csv", index=False)
print(f"\nğŸ’¾ è©³ç´°çµæœå·²å„²å­˜è‡³: comprehensive_model_results.csv")

print(f"\n" + "=" * 80)
print("ğŸ¯ æ¨¡å‹æ”¹é€²å®Œæˆï¼")
print("=" * 80)