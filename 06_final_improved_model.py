#!/usr/bin/env python3
"""
æœ€çµ‚æ”¹é€²ç‰ˆæœ¬ï¼šåŸºæ–¼01_load_inspect.pyæ¶æ§‹
âœ… æ“´å±•æ‰€æœ‰ç›®æ¨™è®Šæ•¸
âœ… çµ„åˆæŒ‡ç´‹ + åˆ†å­æè¿°ç¬¦
âœ… äº¤å‰é©—è­‰
âœ… å®Œæ•´è©•ä¼°æŒ‡æ¨™
"""

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem, MACCSkeys, Descriptors, rdMolDescriptors
from rdkit.Chem.MACCSkeys import GenMACCSKeys
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ æœ€çµ‚æ”¹é€²ç‰ˆèšåˆç‰©æ€§è³ªé æ¸¬æ¨¡å‹")
print("=" * 50)
print("åŸºæ–¼ 01_load_inspect.py æ¶æ§‹çš„å®Œæ•´æ”¹é€²")

# è®€å…¥è³‡æ–™
train = pd.read_csv("./data/train.csv")
test = pd.read_csv("./data/test.csv")

print(f"è¨“ç·´é›†å¤§å°: {train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {test.shape}")

# SMILES è½‰ mol
train["mol"] = train["SMILES"].apply(Chem.MolFromSmiles)
test["mol"] = test["SMILES"].apply(Chem.MolFromSmiles)

# æ”¹é€²çš„æŒ‡ç´‹çµ„åˆå‡½æ•¸
def compute_enhanced_fingerprints(mol):
    """çµ„åˆMorgan + MACCSæŒ‡ç´‹ + é—œéµæè¿°ç¬¦"""
    # æŒ‡ç´‹éƒ¨åˆ†
    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
    fp2 = GenMACCSKeys(mol)
    
    # é—œéµåˆ†å­æè¿°ç¬¦
    descriptors = [
        Descriptors.MolWt(mol),                    # åˆ†å­é‡ - å½±éŸ¿æ‰€æœ‰ç‰©æ€§
        Descriptors.MolLogP(mol),                  # ç–æ°´æ€§ - å½±éŸ¿FFV, Density
        rdMolDescriptors.CalcTPSA(mol),            # æ¥µæ€§è¡¨é¢ç© - å½±éŸ¿çµæ™¶æ€§
        Descriptors.NumAromaticRings(mol),         # èŠ³é¦™ç’° - å½±éŸ¿Tgå‰›æ€§
        rdMolDescriptors.CalcFractionCSP3(mol),    # sp3æ¯”ä¾‹ - å½±éŸ¿æŸ”æ€§
        Descriptors.NumRotatableBonds(mol),        # å¯æ—‹è½‰éµ - å½±éŸ¿Rg
        Descriptors.NumHDonors(mol),               # æ°«éµä¾›é«” - å½±éŸ¿Tc
        Descriptors.NumHAcceptors(mol),            # æ°«éµå—é«” - å½±éŸ¿çµæ™¶
        mol.GetNumHeavyAtoms(),                    # åˆ†å­å¤§å° - å½±éŸ¿å¯†åº¦
        Descriptors.NumRings(mol),                 # ç’°æ•¸ - å½±éŸ¿å‰›æ€§
    ]
    
    # è™•ç†å¯èƒ½çš„ç•°å¸¸å€¼
    descriptors = [0.0 if (pd.isna(x) or np.isinf(x)) else float(x) for x in descriptors]
    
    # çµ„åˆæ‰€æœ‰ç‰¹å¾µ
    return np.concatenate([np.array(fp1), np.array(fp2), np.array(descriptors)])

print("\nè¨ˆç®—å¢å¼·å‹ç‰¹å¾µ...")
X_train = np.array([compute_enhanced_fingerprints(mol) for mol in train["mol"]])
X_test = np.array([compute_enhanced_fingerprints(mol) for mol in test["mol"]])

print(f"ç‰¹å¾µç¶­åº¦: {X_train.shape[1]} (Morgan:2048 + MACCS:167 + Descriptors:10)")

# ç›®æ¨™è®Šæ•¸
targets = ["Tg", "FFV", "Tc", "Density", "Rg"]
y_data = train[targets].copy()

print(f"\nç›®æ¨™è®Šæ•¸çµ±è¨ˆ:")
for target in targets:
    missing = y_data[target].isnull().sum()
    available = y_data[target].notnull().sum()
    percentage = (available / len(y_data)) * 100
    print(f"  {target}: å¯ç”¨ {available} ({percentage:.1f}%), ç¼ºå¤± {missing}")

# æ”¹é€²çš„æ¨¡å‹åƒæ•¸
improved_model = RandomForestRegressor(
    n_estimators=200,        # å¹³è¡¡æ€§èƒ½èˆ‡é€Ÿåº¦
    max_depth=15,           # é˜²æ­¢éæ“¬åˆ
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

# åŸºæº–çµæœ (ä¾†è‡ªåŸå§‹æ¨¡å‹)
baseline_results = {
    'Tg': 0.4573, 'FFV': 0.6520, 'Tc': 0.7046, 
    'Density': 0.7401, 'Rg': 0.6705
}

print(f"\né–‹å§‹è¨“ç·´èˆ‡è©•ä¼°...")
print("=" * 60)

# çµæœå„²å­˜
comprehensive_results = []

for target in targets:
    print(f"\nğŸ¯ ç›®æ¨™è®Šæ•¸: {target}")
    print("-" * 30)
    
    # å–å¾—è©²ç›®æ¨™çš„éç©ºæ¨£æœ¬
    mask = y_data[target].notnull()
    X_target = X_train[mask]
    y_target = y_data[target][mask]
    
    n_samples = len(y_target)
    print(f"å¯ç”¨æ¨£æœ¬æ•¸: {n_samples}")
    
    if n_samples < 50:
        print("âš ï¸  æ¨£æœ¬æ•¸å¤ªå°‘ï¼Œè·³é")
        continue
    
    # åˆ‡åˆ†è¨“ç·´é›†èˆ‡é©—è­‰é›†
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_target, y_target, test_size=0.2, random_state=42
    )
    
    # ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_val_scaled = scaler.transform(X_val)
    
    # è¨“ç·´æ¨¡å‹
    model = RandomForestRegressor(**improved_model.get_params())
    model.fit(X_tr_scaled, y_tr)
    
    # é æ¸¬
    y_pred_tr = model.predict(X_tr_scaled)
    y_pred_val = model.predict(X_val_scaled)
    
    # Hold-outè©•ä¼°æŒ‡æ¨™
    train_r2 = r2_score(y_tr, y_pred_tr)
    val_r2 = r2_score(y_val, y_pred_val)
    val_mse = mean_squared_error(y_val, y_pred_val)
    val_rmse = np.sqrt(val_mse)
    val_mae = mean_absolute_error(y_val, y_pred_val)
    
    # ç›¸å°èª¤å·®
    y_std = y_target.std()
    relative_rmse = val_rmse / y_std
    
    # éæ“¬åˆæª¢æŸ¥
    overfitting = train_r2 - val_r2
    
    # äº¤å‰é©—è­‰
    print("  åŸ·è¡Œäº¤å‰é©—è­‰...")
    X_all_scaled = scaler.fit_transform(X_target)
    cv_r2_scores = cross_val_score(model, X_all_scaled, y_target, cv=5, scoring='r2')
    cv_mse_scores = cross_val_score(model, X_all_scaled, y_target, cv=5, scoring='neg_mean_squared_error')
    
    cv_r2_mean = cv_r2_scores.mean()
    cv_r2_std = cv_r2_scores.std()
    cv_rmse_mean = np.sqrt(-cv_mse_scores.mean())
    
    # èˆ‡åŸºæº–æ¯”è¼ƒ
    baseline = baseline_results.get(target, 0)
    improvement = val_r2 - baseline
    improvement_pct = (improvement / baseline * 100) if baseline > 0 else 0
    
    # å„²å­˜çµæœ
    result = {
        'target': target,
        'n_samples': n_samples,
        'baseline_r2': baseline,
        'holdout_r2': val_r2,
        'cv_r2_mean': cv_r2_mean,
        'cv_r2_std': cv_r2_std,
        'rmse': val_rmse,
        'cv_rmse': cv_rmse_mean,
        'relative_rmse': relative_rmse,
        'overfitting': overfitting,
        'improvement': improvement,
        'improvement_pct': improvement_pct
    }
    comprehensive_results.append(result)
    
    # é¡¯ç¤ºçµæœ
    print(f"  Hold-out RÂ²: {val_r2:.4f}")
    print(f"  äº¤å‰é©—è­‰ RÂ²: {cv_r2_mean:.4f} Â± {cv_r2_std:.4f}")
    print(f"  RMSE: {val_rmse:.4f} (ç›¸å°: {relative_rmse:.3f})")
    print(f"  èˆ‡åŸºæº–æ¯”è¼ƒ: {baseline:.4f} â†’ {val_r2:.4f} ({improvement_pct:+.1f}%)")
    print(f"  éæ“¬åˆç¨‹åº¦: {overfitting:.4f}")

# ===============================
# ç¶œåˆçµæœåˆ†æ
# ===============================
print(f"\n" + "=" * 70)
print("ğŸ“Š å®Œæ•´æ¨¡å‹è©•ä¼°å ±å‘Š")
print("=" * 70)

# è½‰æ›ç‚ºDataFrame
results_df = pd.DataFrame(comprehensive_results)

print(f"\nğŸ† æ€§èƒ½æ‘˜è¦")
print("-" * 50)
print(f"{'ç›®æ¨™':8s} {'æ¨£æœ¬æ•¸':>6s} {'åŸºæº–RÂ²':>8s} {'æ”¹é€²RÂ²':>8s} {'CV RÂ²':>12s} {'æå‡%':>8s}")
print("-" * 50)

total_improvement = 0
count = 0

for _, row in results_df.iterrows():
    print(f"{row['target']:8s} "
          f"{row['n_samples']:6.0f} "
          f"{row['baseline_r2']:8.4f} "
          f"{row['holdout_r2']:8.4f} "
          f"{row['cv_r2_mean']:6.4f}Â±{row['cv_r2_std']:4.3f} "
          f"{row['improvement_pct']:+7.1f}%")
    
    if row['improvement_pct'] != 0:
        total_improvement += row['improvement_pct']
        count += 1

if count > 0:
    avg_improvement = total_improvement / count
    print(f"{'å¹³å‡':8s} {'':6s} {'':8s} {'':8s} {'':12s} {avg_improvement:+7.1f}%")

print(f"\nğŸ” æ¨¡å‹å“è³ªåˆ†æ")
print("-" * 50)

def get_quality_rating(r2, rel_rmse, overfitting):
    """ç¶œåˆå“è³ªè©•åˆ†"""
    if r2 >= 0.8 and rel_rmse <= 0.3 and abs(overfitting) <= 0.05:
        return "å„ªç§€", "ğŸ†"
    elif r2 >= 0.6 and rel_rmse <= 0.5 and abs(overfitting) <= 0.1:
        return "è‰¯å¥½", "âœ…"
    elif r2 >= 0.4 and rel_rmse <= 0.7 and abs(overfitting) <= 0.15:
        return "å¯æ¥å—", "âš ï¸"
    else:
        return "éœ€æ”¹é€²", "âŒ"

print(f"{'ç›®æ¨™':8s} {'RÂ²':>8s} {'ç›¸å°RMSE':>10s} {'éæ“¬åˆ':>8s} {'è©•ç´š':>6s}")
print("-" * 50)

for _, row in results_df.iterrows():
    quality, emoji = get_quality_rating(row['holdout_r2'], row['relative_rmse'], row['overfitting'])
    print(f"{row['target']:8s} "
          f"{row['holdout_r2']:8.4f} "
          f"{row['relative_rmse']:10.4f} "
          f"{row['overfitting']:8.4f} "
          f"{emoji} {quality}")

# ç‰¹å¾µé‡è¦æ€§åˆ†æ (é‡å°è¡¨ç¾æœ€å¥½çš„ç›®æ¨™)
best_target_idx = results_df['holdout_r2'].idxmax()
best_target = results_df.iloc[best_target_idx]['target']

print(f"\nğŸ”¬ æ¨¡å‹è¨ºæ–·")
print("-" * 50)
print(f"è¡¨ç¾æœ€ä½³: {best_target} (RÂ² = {results_df.iloc[best_target_idx]['holdout_r2']:.4f})")

# ç©©å¥æ€§åˆ†æ
cv_stability = results_df['cv_r2_std'].mean()
print(f"äº¤å‰é©—è­‰ç©©å®šæ€§: {cv_stability:.4f} (è¶Šå°è¶Šç©©å®š)")

# éæ“¬åˆåˆ†æ
overfitting_issues = results_df[results_df['overfitting'] > 0.1]
if len(overfitting_issues) > 0:
    print(f"âš ï¸  éæ“¬åˆå•é¡Œ: {', '.join(overfitting_issues['target'].tolist())}")
else:
    print("âœ… ç„¡æ˜é¡¯éæ“¬åˆå•é¡Œ")

print(f"\nğŸ’¡ æ”¹é€²å»ºè­°")
print("-" * 50)

avg_r2 = results_df['holdout_r2'].mean()
if avg_r2 >= 0.7:
    print("ğŸ‰ æ•´é«”è¡¨ç¾å„ªç§€ï¼å»ºè­°ï¼š")
    print("   â€¢ å˜—è©¦é›†æˆå­¸ç¿’ (Voting/Stacking)")
    print("   â€¢ ç²¾ç´°è¶…åƒæ•¸èª¿å„ª")
    print("   â€¢ æ·»åŠ æ›´å¤šé ˜åŸŸç‰¹å¾µ")
elif avg_r2 >= 0.5:
    print("ğŸ‘ è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ï¼š")
    print("   â€¢ å˜—è©¦XGBoost/LightGBM")
    print("   â€¢ ç‰¹å¾µé¸æ“‡å„ªåŒ–")
    print("   â€¢ å¢åŠ æ¨¡å‹è¤‡é›œåº¦")
else:
    print("ğŸ”§ éœ€è¦é‡å¤§æ”¹é€²ï¼š")
    print("   â€¢ æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹")
    print("   â€¢ å˜—è©¦ä¸åŒç®—æ³•")
    print("   â€¢ è€ƒæ…®æ•¸æ“šè³ªé‡")

# å…·é«”ç›®æ¨™å»ºè­°
worst_target = results_df.loc[results_df['holdout_r2'].idxmin(), 'target']
print(f"\né‡å°è¡¨ç¾æœ€å·®çš„ {worst_target}:")
print("   â€¢ è€ƒæ…®é ˜åŸŸç‰¹å®šç‰¹å¾µ")
print("   â€¢ æª¢æŸ¥æ•¸æ“šåˆ†å¸ƒ")
print("   â€¢ å˜—è©¦éç·šæ€§æ¨¡å‹")

# å„²å­˜çµæœ
results_df.to_csv("final_comprehensive_results.csv", index=False)

print(f"\nğŸ’¾ è©³ç´°çµæœå·²å„²å­˜è‡³: final_comprehensive_results.csv")

print(f"\n" + "=" * 70)
print("ğŸ¯ æ”¹é€²å®Œæˆç¸½çµ")
print("=" * 70)
print(f"âœ… ä½¿ç”¨çµ„åˆç‰¹å¾µ: MorganæŒ‡ç´‹ + MACCSæŒ‡ç´‹ + åˆ†å­æè¿°ç¬¦")
print(f"âœ… æ”¹é€²RandomForeståƒæ•¸ï¼Œé˜²æ­¢éæ“¬åˆ")
print(f"âœ… å¯¦æ–½5æŠ˜äº¤å‰é©—è­‰ç¢ºä¿ç©©å¥æ€§")
print(f"âœ… æä¾›å®Œæ•´è©•ä¼°æŒ‡æ¨™èˆ‡è¨ºæ–·")
print(f"âœ… å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")
print("=" * 70)